{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423581a5",
   "metadata": {},
   "source": [
    "# Continuation of IV. Popularity Classification\n",
    "\n",
    "## Model Building (using undersampling):\n",
    "_As mentioned in the previous notebook, I'm performing undersampling to see if I can get a generalized model here without any overfit. Even though some data is lost here, since our dataset is huge (110k+), thought it might be a viable option to experiment._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fb8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7dd3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('cleaneddata_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884a4c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 113549 entries, 0 to 113999\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   duration_ms               113549 non-null  int64  \n",
      " 1   danceability              113549 non-null  float64\n",
      " 2   energy                    113549 non-null  float64\n",
      " 3   loudness                  113549 non-null  float64\n",
      " 4   speechiness               113549 non-null  float64\n",
      " 5   acousticness              113549 non-null  float64\n",
      " 6   instrumentalness          113549 non-null  float64\n",
      " 7   liveness                  113549 non-null  float64\n",
      " 8   valence                   113549 non-null  float64\n",
      " 9   tempo                     113549 non-null  float64\n",
      " 10  key_sin                   113549 non-null  float64\n",
      " 11  key_cos                   113549 non-null  float64\n",
      " 12  mode_1                    113549 non-null  int32  \n",
      " 13  time_signature_1          113549 non-null  int32  \n",
      " 14  time_signature_3          113549 non-null  int32  \n",
      " 15  time_signature_5          113549 non-null  int32  \n",
      " 16  explicit                  113549 non-null  int32  \n",
      " 17  popularity_class_encoded  113549 non-null  int64  \n",
      "dtypes: float64(11), int32(5), int64(2)\n",
      "memory usage: 14.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "232a3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d3aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target and Feature split\n",
    "X = df.drop(['popularity_class_encoded'], axis=1)\n",
    "y = df['popularity_class_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "#Applying RandomUnderSampler to the training data\n",
    "under_sampler = RandomUnderSampler(random_state=0)\n",
    "X_train_under, y_train_under = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "#Models for evaluation (the true, false -> tells us if data has to be scaled or not)\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=1000), True),\n",
    "    (\"KNN\", KNeighborsClassifier(), True),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(), False),\n",
    "    (\"Random Forest\", RandomForestClassifier(), False),\n",
    "    (\"AdaBoost\", AdaBoostClassifier(), False),\n",
    "    (\"Naive Bayes\", GaussianNB(), True),\n",
    "    (\"XGBoost\", XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54afd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "#Dictionary to store each model\n",
    "fitted_models = {}\n",
    "\n",
    "#Scaling features if necessary\n",
    "if any(model[2] for model in models):  #Checking if any model requires scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_under)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "else:\n",
    "    X_train_scaled, X_test_scaled = X_train_under, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c4a6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and evaluating models\n",
    "for name, model, needs_scaling in models:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Fitting the model\n",
    "    model.fit(X_train_scaled if needs_scaling else X_train_under, y_train_under)\n",
    "    \n",
    "    #Storing the fitted model\n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    #Making predictions\n",
    "    y_pred_train = model.predict(X_train_scaled if needs_scaling else X_train_under)\n",
    "    y_pred_test = model.predict(X_test_scaled if needs_scaling else X_test)\n",
    "    \n",
    "    #Evaluating performance\n",
    "    train_accuracy = accuracy_score(y_train_under, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_test, average='weighted')\n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    #Appending results to the list\n",
    "    results_list.append({\"Model\": name, \"Accuracy_Train\": train_accuracy, \n",
    "                         \"Accuracy_Test\": test_accuracy, \"Precision\": precision,\n",
    "                         \"Recall\": recall, \"F1_Score\": f1, \"Time_Taken\": time_taken})\n",
    "\n",
    "results_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b75585b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy_Train</th>\n",
       "      <th>Accuracy_Test</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Time_Taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.428002</td>\n",
       "      <td>0.412359</td>\n",
       "      <td>0.449937</td>\n",
       "      <td>0.412359</td>\n",
       "      <td>0.415682</td>\n",
       "      <td>0.224231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.679923</td>\n",
       "      <td>0.522325</td>\n",
       "      <td>0.541571</td>\n",
       "      <td>0.522325</td>\n",
       "      <td>0.522158</td>\n",
       "      <td>19.903474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.991489</td>\n",
       "      <td>0.585792</td>\n",
       "      <td>0.604294</td>\n",
       "      <td>0.585792</td>\n",
       "      <td>0.586554</td>\n",
       "      <td>1.926924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.991489</td>\n",
       "      <td>0.658007</td>\n",
       "      <td>0.670067</td>\n",
       "      <td>0.658007</td>\n",
       "      <td>0.660145</td>\n",
       "      <td>46.634097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.455271</td>\n",
       "      <td>0.426538</td>\n",
       "      <td>0.466915</td>\n",
       "      <td>0.426538</td>\n",
       "      <td>0.430162</td>\n",
       "      <td>5.361146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.405392</td>\n",
       "      <td>0.349185</td>\n",
       "      <td>0.457924</td>\n",
       "      <td>0.349185</td>\n",
       "      <td>0.321734</td>\n",
       "      <td>0.100839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.723956</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>0.593868</td>\n",
       "      <td>0.567151</td>\n",
       "      <td>0.570957</td>\n",
       "      <td>2.441031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy_Train  Accuracy_Test  Precision    Recall  \\\n",
       "0  Logistic Regression        0.428002       0.412359   0.449937  0.412359   \n",
       "1                  KNN        0.679923       0.522325   0.541571  0.522325   \n",
       "2        Decision Tree        0.991489       0.585792   0.604294  0.585792   \n",
       "3        Random Forest        0.991489       0.658007   0.670067  0.658007   \n",
       "4             AdaBoost        0.455271       0.426538   0.466915  0.426538   \n",
       "5          Naive Bayes        0.405392       0.349185   0.457924  0.349185   \n",
       "6              XGBoost        0.723956       0.567151   0.593868  0.567151   \n",
       "\n",
       "   F1_Score  Time_Taken  \n",
       "0  0.415682    0.224231  \n",
       "1  0.522158   19.903474  \n",
       "2  0.586554    1.926924  \n",
       "3  0.660145   46.634097  \n",
       "4  0.430162    5.361146  \n",
       "5  0.321734    0.100839  \n",
       "6  0.570957    2.441031  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2cd58",
   "metadata": {},
   "source": [
    "## Insights for the under-sampled modelling: \n",
    "- In comparision to the over-sampled results, we see that the under sampled results performed similarly but slightly worser. Reduced test accuracy in all the models with the train accuracy almost similar. \n",
    "- Over-sampled Random Forest before hyper-parameter tuning gave test accuracy of 69.8% while the under-sampled train accuracy gives 65.8%, clear reduction of accuracy. Similarly, over-sampled XGBoost before hyper-parameter tuning gave test accuracy of 59.4% and while under-sampled XGBoost gives 56.7%, reduction again.  \n",
    "- Not performing hyper-parameter tuning here since I'm convinced that these models will not perform better than the over-sampled models as we've seen how the test accuracy dropped from the over-sampled models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020307f5",
   "metadata": {},
   "source": [
    "# V. Result comparison between Regression vs Classification\n",
    "\n",
    "## Model Complexity and Interpretability\n",
    "- **Regression**: Predicted continuous popularity scores, dealing with non-linear relationships and skewness. Best model (Random Forest) achieved moderate R²=0.54 with significant overfitting.\n",
    "- **Classification**: Simplified prediction by categorizing popularity into classes based on the distribution of popularity. Tuned Random Forest and XGBoost models achieved ~70% accuracy with balanced scores, demonstrating better performance.\n",
    "\n",
    "## Accuracy and Generalization\n",
    "- **Regression**: Struggled with high errors and low test R² values while high train R² value indicating poor generalization.\n",
    "- **Classification**: Showed better accuracy and generalization, particularly after hyper-parameter tuning and class balancing with SMOTE.\n",
    "\n",
    "## Model Training and Evaluation\n",
    "- **Regression**: Involved feature engineering and transformations but still showed overfitting.\n",
    "- **Classification**: Benefitted from SMOTE for class balance and hyper-parameter tuning, leading to models with better discrimination capabilities.\n",
    "\n",
    "## Feature Importance and Insights\n",
    "- **Regression**: Identified features like acousticness and danceability as important, but overfitting hindered accurate predictions.\n",
    "- **Classification**: Provided clearer insights into feature influence on popularity classes, with balanced importance across features.\n",
    "\n",
    "## Computational Efficiency\n",
    "- **Regression**: Required significant resources for training and tuning without substantial performance improvements.\n",
    "- **Classification**: Demanded computational power for tuning but yielded better performance, justifying the investment.\n",
    "\n",
    "Classification outperformed regression in predicting song popularity across accuracy, generalization, and interpretability. It effectively addressed class imbalance and provided actionable insights, making it the preferred approach for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ead225",
   "metadata": {},
   "source": [
    "# VI. Conclusion\n",
    "\n",
    "After extensive data analysis, feature engineering, and model evaluation through both regression and classification approaches, the analysis concludes with the recommendation of a Random Forest classification model, refined through hyper-parameter tuning and oversampling techniques to handle class imbalances effectively. This model demonstrates a robust ability to classify songs into their respective popularity categories, with substantial accuracy, precision, recall, and F1 score metrics.\n",
    "\n",
    "## Key Insights:\n",
    "\n",
    "- **Musical Features Matter**: Features such as acousticness, duration, danceability, and energy levels play significant roles in predicting song popularity. This insight can guide artists and producers in creating music that aligns with current trends and preferences.\n",
    "- **Handling Class Imbalance**: The use of oversampling (SMOTE) proved more effective than undersampling in this context, leading to better model performance. This strategy ensures that the model learns adequately from all classes, reducing bias towards more common outcomes.\n",
    "- **Predictive Modeling as a Tool**: The developed model can serve as a strategic tool for stakeholders in the music industry to predict song popularity with a reasonable degree of accuracy. It can complement existing decision-making processes, from A&R (Artists and Repertoire) decisions to marketing strategy formulation.\n",
    "\n",
    "## Strategic Recommendations:\n",
    "\n",
    "- **Data-Driven A&R Decisions**: Utilize the model's insights in the artist signing and song selection process, favoring songs with characteristics that align with patterns of higher popularity classes.\n",
    "- **Tailored Marketing Strategies**: Allocate marketing resources more efficiently by focusing on songs predicted to achieve medium to high popularity, using targeted promotion strategies to maximize their market impact.\n",
    "- **Creative Guidance**: Artists and songwriters can use the model's insights into influential features to guide their creative process, potentially increasing their songs' appeal to a broader audience.\n",
    "- **Continuous Improvement and Adaptation**: Regularly update the model with new data to adapt to changing musical trends and preferences, ensuring its predictions remain relevant and accurate.\n",
    "\n",
    "## Future Directions:\n",
    "\n",
    "- Exploring advanced machine learning and deep learning techniques to enhance model performance further.\n",
    "- Integrating sentiment analysis of lyrics and social media trends to add another dimension to the predictive capabilities.\n",
    "- Expanding the model to include regional and genre-specific popularity predictions, offering more insights for targeted marketing strategies.\n",
    "\n",
    "In conclusion, the predictive model offers valuable insights and a strategic advantage in the highly competitive music industry, enabling stakeholders to make informed decisions based on data-driven predictions of song popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf356b",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "* Shearer, C. (2000). The CRISP-DM model: The new blueprint for data mining. Journal of Data Warehousing, 5(4), 13-22. This article introduces the CRISP-DM (Cross-Industry Standard Process for Data Mining) methodology, which is a structured approach to planning and executing data mining projects. It details the six phases of the process: business understanding, data understanding, data preparation, modeling, evaluation, and deployment, providing a comprehensive framework for managing data mining efforts and enhancing the overall efficiency and effectiveness of projects.\n",
    "\n",
    "* Brownlee, J. (2020). Imbalanced Classification with Python: Better Metrics, Balance Skewed Classes, Cost-Sensitive Learning. Machine Learning Mastery. This reference provided insights into handling class imbalances, crucial for developing the predictive model.\n",
    "\n",
    "* James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer. This textbook was invaluable for understanding the statistical underpinnings of the machine learning models used.\n",
    "\n",
    "* Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825-2830. This paper provided a comprehensive overview of the Scikit-learn library, which was instrumental in model development and evaluation.\n",
    "\n",
    "* Torgo, L. (2010). Data Mining with R, learning with case studies. Chapman and Hall/CRC. This book provided practical examples of data mining that influenced the exploratory data analysis and feature engineering phases of the project.\n",
    "\n",
    "* Chollet, F. (2018). Deep Learning with Python. Manning Publications. This book offered foundational knowledge on deep learning techniques that could enhance future model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
